{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1748d511-3596-4549-92b0-4077d39479b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c5f329d-b995-4817-b358-cf86f0e8982e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if GPU is available\n",
    "print(torch.cuda.device_count())  # Number of GPUs available\n",
    "print(torch.cuda.get_device_name(0))  # Name of the first GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82dfb41e-687f-4585-8c36-c73397c675da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),  # Random rotations\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Adjust colors\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random crop with scaling\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize as per ResNet\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(root='C:\\\\Users\\\\redfr\\\\Downloads\\\\archive\\\\for-2sec\\\\for-2seconds\\\\training-images', transform=transform_train)\n",
    "\n",
    "\n",
    "# Compute class counts dynamically\n",
    "all_labels = [label for _, label in train_data.samples]  # Extract all labels\n",
    "class_counts = Counter(all_labels)  # Count occurrences of each class\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = 1. / torch.tensor([class_counts[cls] for cls in sorted(class_counts.keys())], dtype=torch.float)\n",
    "\n",
    "# Compute sample weights\n",
    "sample_weights = [class_weights[label] for _, label in train_data.samples]\n",
    "\n",
    "# Define the WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Update DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=32, sampler=sampler)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_data = datasets.ImageFolder(root='C:\\\\Users\\\\redfr\\\\Downloads\\\\archive\\\\for-2sec\\\\for-2seconds\\\\validation-images', transform=transform_val)\n",
    "\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3475cc9a-d31f-4895-95d9-ed09b08cd3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\redfr\\elevenvenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\redfr\\elevenvenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "# Load a pre-trained ResNet model\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Unfreeze some layers for fine-tuning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "# Unfreeze the last few layers\n",
    "for param in model.layer4.parameters():  # Unfreeze the last residual block\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.fc.requires_grad = True  # Unfreeze the fully connected head (already modified)\n",
    "\n",
    "# Modify the loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "class CustomClassificationHead(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(CustomClassificationHead, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_features, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 1)  # Single output unit (logits)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Replace the ResNet fc layer with the custom classification head\n",
    "model.fc = CustomClassificationHead(model.fc.in_features)\n",
    "\n",
    "# Send the model to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6dc320d-7fb8-4c73-9552-328b0ca4c8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\redfr\\elevenvenv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Training Loss: 0.6777931577280948\n",
      "Validation Loss: 0.6524626960915126\n",
      "Epoch 2/40, Training Loss: 0.6005927282012573\n",
      "Validation Loss: 0.5482822649934319\n",
      "Epoch 3/40, Training Loss: 0.4796047139085949\n",
      "Validation Loss: 0.42730511423577083\n",
      "Epoch 4/40, Training Loss: 0.3859639907387902\n",
      "Validation Loss: 0.33171024081412326\n",
      "Epoch 5/40, Training Loss: 0.3266810616243895\n",
      "Validation Loss: 0.27653290482049575\n",
      "Epoch 6/40, Training Loss: 0.2911907239077566\n",
      "Validation Loss: 0.22599692567345803\n",
      "Epoch 7/40, Training Loss: 0.26751849716806303\n",
      "Validation Loss: 0.2189605226844884\n",
      "Epoch 8/40, Training Loss: 0.242044109655054\n",
      "Validation Loss: 0.19413894176315727\n",
      "Epoch 9/40, Training Loss: 0.2158321804394711\n",
      "Validation Loss: 0.16764079473829002\n",
      "Epoch 10/40, Training Loss: 0.21332471960946678\n",
      "Validation Loss: 0.14069254286168667\n",
      "Epoch 11/40, Training Loss: 0.19377399597410752\n",
      "Validation Loss: 0.1412876317005479\n",
      "Epoch 12/40, Training Loss: 0.18234728494961694\n",
      "Validation Loss: 0.12022504532772503\n",
      "Epoch 13/40, Training Loss: 0.1800487871607872\n",
      "Validation Loss: 0.11223600582962626\n",
      "Epoch 14/40, Training Loss: 0.16919407850340248\n",
      "Validation Loss: 0.09844193578268705\n",
      "Epoch 15/40, Training Loss: 0.16224005366558342\n",
      "Validation Loss: 0.09370567963531848\n",
      "Epoch 16/40, Training Loss: 0.16239506377917406\n",
      "Validation Loss: 0.08803975829080249\n",
      "Epoch 17/40, Training Loss: 0.1523073127182111\n",
      "Validation Loss: 0.08520866223098186\n",
      "Epoch 18/40, Training Loss: 0.14541815279277565\n",
      "Validation Loss: 0.07497457933894704\n",
      "Epoch 19/40, Training Loss: 0.14118458815686216\n",
      "Validation Loss: 0.08390521371130193\n",
      "Epoch 20/40, Training Loss: 0.13594063867130732\n",
      "Validation Loss: 0.07134442025104935\n",
      "Epoch 21/40, Training Loss: 0.13492867958815066\n",
      "Validation Loss: 0.06480292487219813\n",
      "Epoch 22/40, Training Loss: 0.13025336479022245\n",
      "Validation Loss: 0.06517745239471787\n",
      "Epoch 23/40, Training Loss: 0.12709573813174901\n",
      "Validation Loss: 0.06658093741142683\n",
      "Epoch 24/40, Training Loss: 0.11602678999424117\n",
      "Validation Loss: 0.06058029727905654\n",
      "Epoch 25/40, Training Loss: 0.12155575233189819\n",
      "Validation Loss: 0.05779493554087167\n",
      "Epoch 26/40, Training Loss: 0.12123401873921912\n",
      "Validation Loss: 0.054959723370128805\n",
      "Epoch 27/40, Training Loss: 0.11893728964064841\n",
      "Validation Loss: 0.06064932278535339\n",
      "Epoch 28/40, Training Loss: 0.11392732304936484\n",
      "Validation Loss: 0.05834382657338394\n",
      "Epoch 29/40, Training Loss: 0.10827332868473072\n",
      "Validation Loss: 0.048664929816143565\n",
      "Epoch 30/40, Training Loss: 0.1095046920561204\n",
      "Validation Loss: 0.05195900381364849\n",
      "Epoch 31/40, Training Loss: 0.10805687795497597\n",
      "Validation Loss: 0.04659633213403017\n",
      "Epoch 32/40, Training Loss: 0.1019664277552757\n",
      "Validation Loss: 0.07981718416336213\n",
      "Epoch 33/40, Training Loss: 0.11430980922607385\n",
      "Validation Loss: 0.04505735033537062\n",
      "Epoch 34/40, Training Loss: 0.0992383801184373\n",
      "Validation Loss: 0.0436173320925805\n",
      "Epoch 35/40, Training Loss: 0.10127195809462727\n",
      "Validation Loss: 0.04084533136965853\n",
      "Epoch 36/40, Training Loss: 0.09826206583828075\n",
      "Validation Loss: 0.04205945582819705\n",
      "Epoch 37/40, Training Loss: 0.09535306594515693\n",
      "Validation Loss: 0.04951500467788637\n",
      "Epoch 38/40, Training Loss: 0.09740589622160936\n",
      "Validation Loss: 0.05144610073877854\n",
      "Epoch 39/40, Training Loss: 0.09634536757289476\n",
      "Validation Loss: 0.03867679830964948\n",
      "Epoch 40/40, Training Loss: 0.09896519538697103\n",
      "Validation Loss: 0.0391108116129769\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(all_labels), y=all_labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])  # Adjust for \"fake-images\"\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6, weight_decay=1e-4)  # Add weight decay\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.1, verbose=True)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 40\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Reshape labels to match the output shape and ensure they're of type float\n",
    "        labels = labels.view(-1, 1).float()  # Reshape to (batch_size, 1) and convert to float\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print average loss after each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # Now evaluate on the validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.view(-1, 1).float()  # Reshape to match model output\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Print the validation loss after each epoch\n",
    "    print(f\"Validation Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "    # Step the scheduler with the average validation loss\n",
    "    scheduler.step(val_loss/len(val_loader))  # Pass average validation loss to scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3126c79d-8a7b-4937-9322-40fdde093326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 68.60%\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "test_data = datasets.ImageFolder(root='C:\\\\Users\\\\redfr\\\\Downloads\\\\archive\\\\for-2sec\\\\for-2seconds\\\\testing-images', transform=transform_val)\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize variables to track accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = model(inputs)\n",
    "        predicted = torch.sigmoid(outputs).round()  # Binary predictions (0 or 1)\n",
    "\n",
    "        # Ensure labels are in the same format (0 or 1)\n",
    "        labels = labels.view(-1, 1)  # Reshape labels to (batch_size, 1) if needed\n",
    "\n",
    "        # Update correct and total counts\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5ed40ff-fbb3-4a4f-ad7a-61ee2c6645b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[231 314]\n",
      " [ 28 516]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " fake-images       0.89      0.42      0.57       545\n",
      " real-images       0.62      0.95      0.75       544\n",
      "\n",
      "    accuracy                           0.69      1089\n",
      "   macro avg       0.76      0.69      0.66      1089\n",
      "weighted avg       0.76      0.69      0.66      1089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predicted = torch.sigmoid(outputs).round()\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Optional: Generate classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=test_data.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a046c-b899-4366-8c0d-927e046b334f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (elevenvenv)",
   "language": "python",
   "name": "elevenvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
